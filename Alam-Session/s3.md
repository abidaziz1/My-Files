# AWS STORAGE LABS: HANDS-ON PRACTICE GUIDE

---

## ğŸ§ª LAB OVERVIEW

This guide provides step-by-step instructions for essential AWS storage labs covering S3, EBS, and EFS. Each lab includes both **AWS Console (Manual)** and **AWS CLI/CloudShell** methods.

**Prerequisites:**
- Active AWS account (Free Tier eligible)
- Basic understanding of AWS Console navigation
- AWS CLI configured (for CLI labs)

---

## LAB 1: S3 BUCKET BASICS

### ğŸ“‹ Lab Objectives
- Create an S3 bucket
- Upload and manage objects
- Configure public access
- Delete buckets and objects

---

### METHOD 1: AWS CONSOLE (MANUAL)

#### Step 1: Create S3 Bucket

```
1. Log into AWS Management Console
2. Navigate to Services â†’ Storage â†’ S3
3. Click "Create bucket"

Configuration:
â”œâ”€ Bucket name: my-practice-bucket-[your-initials]-[random-number]
â”‚  Example: my-practice-bucket-ja-2024
â”‚  (Must be globally unique)
â”œâ”€ Region: Select closest region (e.g., us-east-1)
â”œâ”€ Object Ownership: ACLs disabled (recommended)
â”œâ”€ Block Public Access: Keep all 4 checkboxes CHECKED (default)
â”œâ”€ Bucket Versioning: Disabled (for now)
â”œâ”€ Encryption: Server-side encryption (SSE-S3) - default
â””â”€ Click "Create bucket"

âœ… Bucket created successfully!
```

#### Step 2: Upload Files

```
1. Click on your bucket name
2. Click "Upload" button
3. Click "Add files"
4. Select a test file from your computer
   (e.g., image.jpg or test.txt)
5. Review settings:
   â”œâ”€ Storage class: Standard (default)
   â”œâ”€ Server-side encryption: Enabled
   â””â”€ Permissions: Default
6. Click "Upload"

âœ… File uploaded!
```

#### Step 3: View and Access Object

```
1. Click on uploaded object name
2. Object details displayed:
   â”œâ”€ Object URL: https://bucket-name.s3.region.amazonaws.com/filename
   â”œâ”€ Type: image/jpeg or text/plain
   â”œâ”€ Size: [file size]
   â”œâ”€ Storage class: STANDARD
   â””â”€ Encryption: AES-256

3. Try opening Object URL in new tab
   â†’ Result: Access Denied (expected - bucket is private)
```

#### Step 4: Make Object Public

```
1. Select the object (checkbox)
2. Click "Actions" dropdown
3. Select "Make public using ACL"
4. Confirm by clicking "Make public"

âš ï¸ Warning appears: "Objects can be public"

5. If blocked, need to change bucket settings:
   a. Go back to bucket
   b. Click "Permissions" tab
   c. Edit "Block public access"
   d. Uncheck "Block all public access"
   e. Confirm by typing "confirm"
   f. Save changes

6. Return to object and make public again
7. Open Object URL â†’ âœ… File now accessible!
```

#### Step 5: Delete Object and Bucket

```
DELETE OBJECT:
1. Select object (checkbox)
2. Click "Delete"
3. Type "permanently delete" to confirm
4. Click "Delete objects"

DELETE BUCKET:
1. Go back to S3 buckets list
2. Select bucket (checkbox)
3. Click "Delete"
4. Type bucket name to confirm
5. Click "Delete bucket"

âš ï¸ Note: Bucket must be empty to delete
```

---

### METHOD 2: AWS CLI / CLOUDSHELL

#### Setup AWS CLI (if not already configured)

```bash
# Check if AWS CLI is installed
aws --version

# Configure AWS CLI (skip if using CloudShell)
aws configure
# Enter:
# - AWS Access Key ID
# - AWS Secret Access Key
# - Default region (e.g., us-east-1)
# - Default output format (json)
```

#### Step 1: Create S3 Bucket

```bash
# Set variables for reusability
BUCKET_NAME="my-practice-bucket-cli-$RANDOM"
REGION="us-east-1"

# Create bucket
aws s3 mb s3://$BUCKET_NAME --region $REGION

# Output: make_bucket: my-practice-bucket-cli-12345

# Verify bucket creation
aws s3 ls

# Output shows all your buckets including the new one
```

#### Step 2: Upload Files

```bash
# Create a test file
echo "This is a test file for S3 upload" > test.txt

# Upload file to bucket
aws s3 cp test.txt s3://$BUCKET_NAME/

# Output: upload: ./test.txt to s3://my-practice-bucket-cli-12345/test.txt

# Verify upload
aws s3 ls s3://$BUCKET_NAME/

# Output: 2024-02-16 10:30:00  35 test.txt
```

#### Step 3: List Objects and Get Details

```bash
# List all objects in bucket
aws s3 ls s3://$BUCKET_NAME/ --recursive

# Get object metadata
aws s3api head-object \
  --bucket $BUCKET_NAME \
  --key test.txt

# Output shows:
# - Content-Type
# - Content-Length
# - ETag
# - LastModified
```

#### Step 4: Make Object Public (Using ACL)

```bash
# First, remove block public access on bucket
aws s3api put-public-access-block \
  --bucket $BUCKET_NAME \
  --public-access-block-configuration \
  "BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=false,RestrictPublicBuckets=false"

# Make object public
aws s3api put-object-acl \
  --bucket $BUCKET_NAME \
  --key test.txt \
  --acl public-read

# Get public URL
echo "https://$BUCKET_NAME.s3.$REGION.amazonaws.com/test.txt"

# Test access (using curl)
curl https://$BUCKET_NAME.s3.$REGION.amazonaws.com/test.txt

# Output: This is a test file for S3 upload
```

#### Step 5: Download Object

```bash
# Download file from S3
aws s3 cp s3://$BUCKET_NAME/test.txt downloaded-test.txt

# Output: download: s3://my-practice-bucket-cli-12345/test.txt to ./downloaded-test.txt

# Verify content
cat downloaded-test.txt
```

#### Step 6: Sync Local Folder with S3

```bash
# Create a local folder with multiple files
mkdir sync-test
cd sync-test
echo "File 1" > file1.txt
echo "File 2" > file2.txt
echo "File 3" > file3.txt

# Sync folder to S3
aws s3 sync . s3://$BUCKET_NAME/sync-folder/

# Output:
# upload: ./file1.txt to s3://bucket/sync-folder/file1.txt
# upload: ./file2.txt to s3://bucket/sync-folder/file2.txt
# upload: ./file3.txt to s3://bucket/sync-folder/file3.txt

# Verify sync
aws s3 ls s3://$BUCKET_NAME/sync-folder/
```

#### Step 7: Delete Objects and Bucket

```bash
# Delete single object
aws s3 rm s3://$BUCKET_NAME/test.txt

# Delete all objects in folder
aws s3 rm s3://$BUCKET_NAME/sync-folder/ --recursive

# Verify bucket is empty
aws s3 ls s3://$BUCKET_NAME/

# Delete bucket (must be empty)
aws s3 rb s3://$BUCKET_NAME

# Output: remove_bucket: my-practice-bucket-cli-12345

# Verify deletion
aws s3 ls | grep $BUCKET_NAME
# (No output = bucket deleted)
```

---

## LAB 2: S3 VERSIONING

### ğŸ“‹ Lab Objectives
- Enable versioning on S3 bucket
- Upload multiple versions of same file
- View version history
- Restore deleted files using versions

---

### METHOD 1: AWS CONSOLE (MANUAL)

#### Step 1: Create Bucket and Enable Versioning

```
1. Create new S3 bucket: "versioning-practice-bucket-[random]"
2. After bucket creation, click on bucket name
3. Go to "Properties" tab
4. Scroll to "Bucket Versioning" section
5. Click "Edit"
6. Select "Enable"
7. Click "Save changes"

âœ… Versioning enabled!
```

#### Step 2: Upload Initial Version

```
1. Create a text file on desktop: version-test.txt
   Content: "This is version 1"

2. Upload to bucket:
   â”œâ”€ Click "Upload"
   â”œâ”€ Add file: version-test.txt
   â””â”€ Click "Upload"

3. Click on object â†’ Shows current version
```

#### Step 3: Upload Modified Versions

```
1. Modify local file: version-test.txt
   New content: "This is version 2 - Modified"

2. Upload same file again (same name)
3. Bucket now has 2 versions of the file

4. Modify again: "This is version 3 - Final"
5. Upload again

Result: 3 versions of same object
```

#### Step 4: View Version History

```
1. In bucket, toggle "Show versions" button
2. View all versions listed:

version-test.txt (Version ID: xyz789) â† Latest
version-test.txt (Version ID: abc456)
version-test.txt (Version ID: def123) â† Original

Each version has:
â”œâ”€ Unique Version ID
â”œâ”€ Size
â”œâ”€ Storage class
â””â”€ Last modified timestamp
```

#### Step 5: Delete File and Restore

```
DELETE FILE:
1. Select version-test.txt (latest version)
2. Click "Delete"
3. Confirm deletion

Result: Delete marker created (file appears deleted)

VIEW VERSIONS:
1. Toggle "Show versions"
2. See:
   â”œâ”€ Delete marker (Type: Delete marker)
   â””â”€ Previous versions still exist

RESTORE FILE:
1. Select the delete marker
2. Click "Delete" on the delete marker
3. Confirm deletion

Result: âœ… File restored! Latest version reappears
```

#### Step 6: Access Specific Version

```
1. With "Show versions" enabled
2. Click on specific version (not latest)
3. View version details
4. Can download this specific version
5. Object URL includes version ID:
   https://bucket.s3.region.amazonaws.com/file?versionId=abc123
```

---

### METHOD 2: AWS CLI / CLOUDSHELL

#### Step 1: Create Bucket and Enable Versioning

```bash
# Create bucket
BUCKET_NAME="versioning-cli-bucket-$RANDOM"
aws s3 mb s3://$BUCKET_NAME

# Enable versioning
aws s3api put-bucket-versioning \
  --bucket $BUCKET_NAME \
  --versioning-configuration Status=Enabled

# Verify versioning status
aws s3api get-bucket-versioning --bucket $BUCKET_NAME

# Output: {
#     "Status": "Enabled"
# }
```

#### Step 2: Upload Multiple Versions

```bash
# Create version 1
echo "This is version 1" > version-test.txt
aws s3 cp version-test.txt s3://$BUCKET_NAME/

# Create version 2
echo "This is version 2 - Modified" > version-test.txt
aws s3 cp version-test.txt s3://$BUCKET_NAME/

# Create version 3
echo "This is version 3 - Final" > version-test.txt
aws s3 cp version-test.txt s3://$BUCKET_NAME/

# All three uploads create separate versions
```

#### Step 3: List All Versions

```bash
# List all versions of objects
aws s3api list-object-versions \
  --bucket $BUCKET_NAME

# Output shows:
# {
#     "Versions": [
#         {
#             "Key": "version-test.txt",
#             "VersionId": "xyz789...",
#             "IsLatest": true,
#             "LastModified": "2024-02-16T10:30:00.000Z",
#             "Size": 25
#         },
#         {
#             "Key": "version-test.txt",
#             "VersionId": "abc456...",
#             "IsLatest": false,
#             "LastModified": "2024-02-16T10:25:00.000Z",
#             "Size": 30
#         },
#         {
#             "Key": "version-test.txt",
#             "VersionId": "def123...",
#             "IsLatest": false,
#             "LastModified": "2024-02-16T10:20:00.000Z",
#             "Size": 18
#         }
#     ]
# }
```

#### Step 4: Download Specific Version

```bash
# Get specific version ID (from list above)
VERSION_ID="abc456..."

# Download specific version
aws s3api get-object \
  --bucket $BUCKET_NAME \
  --key version-test.txt \
  --version-id $VERSION_ID \
  downloaded-v2.txt

# Verify content
cat downloaded-v2.txt
# Output: This is version 2 - Modified
```

#### Step 5: Delete and Restore

```bash
# Delete object (creates delete marker)
aws s3 rm s3://$BUCKET_NAME/version-test.txt

# List versions (shows delete marker)
aws s3api list-object-versions --bucket $BUCKET_NAME

# Output includes DeleteMarkers section:
# "DeleteMarkers": [
#     {
#         "Key": "version-test.txt",
#         "VersionId": "del123...",
#         "IsLatest": true
#     }
# ]

# Delete the delete marker to restore file
DELETE_MARKER_ID="del123..."
aws s3api delete-object \
  --bucket $BUCKET_NAME \
  --key version-test.txt \
  --version-id $DELETE_MARKER_ID

# File restored! Latest version accessible again
```

#### Step 6: Suspend Versioning

```bash
# Suspend versioning (cannot disable completely)
aws s3api put-bucket-versioning \
  --bucket $BUCKET_NAME \
  --versioning-configuration Status=Suspended

# Verify
aws s3api get-bucket-versioning --bucket $BUCKET_NAME

# Output: {
#     "Status": "Suspended"
# }

# Note: Existing versions remain accessible
```

---

## LAB 3: S3 LIFECYCLE MANAGEMENT

### ğŸ“‹ Lab Objectives
- Create lifecycle rules
- Transition objects between storage classes
- Set expiration policies
- Monitor lifecycle transitions

---

### METHOD 1: AWS CONSOLE (MANUAL)

#### Step 1: Create Bucket with Test Objects

```
1. Create bucket: "lifecycle-demo-bucket-[random]"
2. Upload multiple test files:
   â”œâ”€ old-data.txt (simulating old data)
   â”œâ”€ recent-data.txt
   â””â”€ archive-data.txt

3. Enable versioning (for version lifecycle rules)
```

#### Step 2: Create Lifecycle Rule

```
1. Go to bucket â†’ "Management" tab
2. Click "Create lifecycle rule"

3. Rule configuration:
   â”œâ”€ Rule name: "TransitionToIA"
   â”œâ”€ Rule scope: Apply to all objects
   â””â”€ Acknowledge scope

4. Lifecycle rule actions:
   â˜‘ Transition current versions between storage classes
   â˜‘ Transition noncurrent versions between storage classes
   â˜‘ Expire current versions of objects
   â˜‘ Delete noncurrent versions of objects
```

#### Step 3: Configure Transitions

```
CURRENT VERSION TRANSITIONS:

Transition 1:
â”œâ”€ After: 30 days
â””â”€ Storage class: Standard-IA

Transition 2:
â”œâ”€ After: 60 days
â””â”€ Storage class: Intelligent-Tiering

Transition 3:
â”œâ”€ After: 90 days
â””â”€ Storage class: Glacier Flexible Retrieval

Transition 4:
â”œâ”€ After: 180 days
â””â”€ Storage class: Glacier Deep Archive

NONCURRENT VERSION TRANSITIONS:

Transition noncurrent versions:
â”œâ”€ After: 30 days â†’ Standard-IA
â”œâ”€ After: 60 days â†’ Glacier Flexible Retrieval
â””â”€ Delete after: 365 days
```

#### Step 4: Set Expiration

```
EXPIRATION SETTINGS:

Current versions:
â”œâ”€ Expire current versions: 730 days (2 years)

Delete markers:
â”œâ”€ Delete expired delete markers: â˜‘ Enable

Incomplete multipart uploads:
â”œâ”€ Delete after: 7 days
```

#### Step 5: Review and Create

```
1. Review all settings:

Rule Summary:
â”œâ”€ Day 0: Standard
â”œâ”€ Day 30: Standard-IA
â”œâ”€ Day 60: Intelligent-Tiering
â”œâ”€ Day 90: Glacier Flexible
â”œâ”€ Day 180: Glacier Deep Archive
â””â”€ Day 730: Expire

2. Click "Create rule"

âœ… Lifecycle rule created!
```

---

### METHOD 2: AWS CLI / CLOUDSHELL

#### Step 1: Create Bucket

```bash
BUCKET_NAME="lifecycle-cli-bucket-$RANDOM"
aws s3 mb s3://$BUCKET_NAME

# Enable versioning (required for version lifecycle)
aws s3api put-bucket-versioning \
  --bucket $BUCKET_NAME \
  --versioning-configuration Status=Enabled
```

#### Step 2: Create Lifecycle Configuration JSON

```bash
# Create lifecycle policy file
cat > lifecycle-policy.json <<'EOF'
{
  "Rules": [
    {
      "Id": "TransitionRule",
      "Status": "Enabled",
      "Filter": {
        "Prefix": ""
      },
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 60,
          "StorageClass": "INTELLIGENT_TIERING"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        },
        {
          "Days": 180,
          "StorageClass": "DEEP_ARCHIVE"
        }
      ],
      "NoncurrentVersionTransitions": [
        {
          "NoncurrentDays": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "NoncurrentDays": 90,
          "StorageClass": "GLACIER"
        }
      ],
      "NoncurrentVersionExpiration": {
        "NoncurrentDays": 365
      },
      "Expiration": {
        "Days": 730
      },
      "AbortIncompleteMultipartUpload": {
        "DaysAfterInitiation": 7
      }
    }
  ]
}
EOF
```

#### Step 3: Apply Lifecycle Policy

```bash
# Apply lifecycle configuration
aws s3api put-bucket-lifecycle-configuration \
  --bucket $BUCKET_NAME \
  --lifecycle-configuration file://lifecycle-policy.json

# Verify configuration
aws s3api get-bucket-lifecycle-configuration \
  --bucket $BUCKET_NAME

# Output shows the applied lifecycle rules
```

#### Step 4: Upload Test Files

```bash
# Upload files with different dates (for testing)
echo "Old data" > old-data.txt
aws s3 cp old-data.txt s3://$BUCKET_NAME/

echo "Recent data" > recent-data.txt
aws s3 cp recent-data.txt s3://$BUCKET_NAME/

echo "Archive data" > archive-data.txt
aws s3 cp archive-data.txt s3://$BUCKET_NAME/
```

#### Step 5: Monitor Lifecycle Transitions

```bash
# Check object storage class
aws s3api head-object \
  --bucket $BUCKET_NAME \
  --key old-data.txt

# Output includes:
# "StorageClass": "STANDARD"
# (Will change to STANDARD_IA after 30 days)

# List all objects with storage class
aws s3api list-objects-v2 \
  --bucket $BUCKET_NAME \
  --query 'Contents[].[Key, StorageClass, LastModified]' \
  --output table
```

---

## LAB 4: CROSS-REGION REPLICATION (CRR)

### ğŸ“‹ Lab Objectives
- Set up source and destination buckets in different regions
- Enable versioning on both buckets
- Configure replication rules
- Test automatic replication

---

### METHOD 1: AWS CONSOLE (MANUAL)

#### Step 1: Create Source Bucket

```
1. Create bucket in us-east-1 (N. Virginia):
   â”œâ”€ Name: "crr-source-bucket-[random]"
   â”œâ”€ Region: us-east-1
   â””â”€ Enable versioning: YES

2. Upload test file: source-file.txt
```

#### Step 2: Create Destination Bucket

```
1. Create bucket in us-west-2 (Oregon):
   â”œâ”€ Name: "crr-destination-bucket-[random]"
   â”œâ”€ Region: us-west-2
   â””â”€ Enable versioning: YES (REQUIRED!)

2. Leave bucket empty (will replicate from source)
```

#### Step 3: Configure Replication Rule

```
1. Go to source bucket (us-east-1)
2. Click "Management" tab
3. Scroll to "Replication rules"
4. Click "Create replication rule"

5. Rule configuration:
   â”œâ”€ Rule name: "ReplicateToWest"
   â”œâ”€ Status: Enabled
   â””â”€ Priority: 1

6. Source:
   â”œâ”€ Rule scope: Apply to all objects
   â””â”€ (Or filter by prefix/tags if needed)

7. Destination:
   â”œâ”€ Bucket name: crr-destination-bucket-[random]
   â”œâ”€ Destination bucket in this account
   â””â”€ Region: us-west-2

8. IAM Role:
   â”œâ”€ Create new role (AWS creates automatically)
   â””â”€ Role name: s3crr_role_for_bucket

9. Encryption: Keep default (replicate with SSE-S3)

10. Destination storage class:
    â”œâ”€ Keep same storage class
    â””â”€ (Or select different class to save costs)

11. Additional options:
    â”œâ”€ Replication Time Control (RTC): Disabled
    â”œâ”€ Replication metrics: Disabled
    â””â”€ Delete marker replication: Disabled (default)

12. Click "Save"
```

#### Step 4: Test Replication

```
TEST 1: Upload New File to Source
1. In source bucket, upload: new-file.txt
2. Wait 1-2 minutes
3. Go to destination bucket (us-west-2)
4. Refresh â†’ âœ… new-file.txt appears!

TEST 2: Modify Existing File
1. Modify source-file.txt (upload new version)
2. Check destination bucket
3. âœ… New version replicated!

TEST 3: Upload to Destination
1. Upload: dest-only-file.txt to destination bucket
2. Check source bucket
3. âŒ File NOT replicated back
   (Replication is ONE-WAY only!)
```

#### Step 5: Verify Replication Status

```
1. In source bucket, click on replicated object
2. Go to "Properties" tab
3. Scroll to "Replication status"
4. Shows:
   â”œâ”€ Replication status: COMPLETED
   â”œâ”€ Destination bucket: arn:aws:s3:::destination-bucket
   â””â”€ Destination storage class: STANDARD
```

---

### METHOD 2: AWS CLI / CLOUDSHELL

#### Step 1: Create Buckets

```bash
# Source bucket (us-east-1)
SOURCE_BUCKET="crr-source-cli-$RANDOM"
aws s3 mb s3://$SOURCE_BUCKET --region us-east-1

# Enable versioning on source
aws s3api put-bucket-versioning \
  --bucket $SOURCE_BUCKET \
  --versioning-configuration Status=Enabled

# Destination bucket (us-west-2)
DEST_BUCKET="crr-dest-cli-$RANDOM"
aws s3 mb s3://$DEST_BUCKET --region us-west-2

# Enable versioning on destination
aws s3api put-bucket-versioning \
  --bucket $DEST_BUCKET \
  --versioning-configuration Status=Enabled
```

#### Step 2: Create IAM Role for Replication

```bash
# Get AWS account ID
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# Create trust policy for S3
cat > trust-policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

# Create IAM role
aws iam create-role \
  --role-name S3-CRR-Role \
  --assume-role-policy-document file://trust-policy.json

# Create permission policy
cat > replication-policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetReplicationConfiguration",
        "s3:ListBucket"
      ],
      "Resource": "arn:aws:s3:::$SOURCE_BUCKET"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObjectVersionForReplication",
        "s3:GetObjectVersionAcl"
      ],
      "Resource": "arn:aws:s3:::$SOURCE_BUCKET/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:ReplicateObject",
        "s3:ReplicateDelete"
      ],
      "Resource": "arn:aws:s3:::$DEST_BUCKET/*"
    }
  ]
}
EOF

# Attach policy to role
aws iam put-role-policy \
  --role-name S3-CRR-Role \
  --policy-name S3-CRR-Policy \
  --policy-document file://replication-policy.json
```

#### Step 3: Configure Replication

```bash
# Create replication configuration
cat > replication-config.json <<EOF
{
  "Role": "arn:aws:iam::$ACCOUNT_ID:role/S3-CRR-Role",
  "Rules": [
    {
      "Status": "Enabled",
      "Priority": 1,
      "DeleteMarkerReplication": {
        "Status": "Disabled"
      },
      "Filter": {},
      "Destination": {
        "Bucket": "arn:aws:s3:::$DEST_BUCKET"
      }
    }
  ]
}
EOF

# Apply replication configuration
aws s3api put-bucket-replication \
  --bucket $SOURCE_BUCKET \
  --replication-configuration file://replication-config.json

# Verify configuration
aws s3api get-bucket-replication --bucket $SOURCE_BUCKET
```

#### Step 4: Test Replication

```bash
# Upload file to source bucket
echo "Test CRR replication" > crr-test.txt
aws s3 cp crr-test.txt s3://$SOURCE_BUCKET/ --region us-east-1

# Wait 30-60 seconds

# Check destination bucket
aws s3 ls s3://$DEST_BUCKET/ --region us-west-2

# Output should show: crr-test.txt

# Download from destination to verify
aws s3 cp s3://$DEST_BUCKET/crr-test.txt replicated-file.txt --region us-west-2
cat replicated-file.txt

# Output: Test CRR replication âœ…
```

#### Step 5: Check Replication Status

```bash
# Get replication status of object
aws s3api head-object \
  --bucket $SOURCE_BUCKET \
  --key crr-test.txt \
  --region us-east-1

# Output includes:
# "ReplicationStatus": "COMPLETED"
```

---

## LAB 5: AMAZON EFS (ELASTIC FILE SYSTEM)

### ğŸ“‹ Lab Objectives
- Create EFS file system
- Launch EC2 instances in multiple AZs
- Mount EFS on instances
- Test shared file access

---

### METHOD 1: AWS CONSOLE (MANUAL)

#### Step 1: Create EFS File System

```
1. Navigate to Services â†’ Storage â†’ EFS
2. Click "Create file system"

3. Configuration:
   â”œâ”€ Name: "shared-efs-lab"
   â”œâ”€ VPC: Default VPC
   â””â”€ Availability and durability: Regional

4. Network access:
   â”œâ”€ Select all availability zones
   â”œâ”€ Subnet: Choose default subnet in each AZ
   â””â”€ Security groups: Default (will modify later)

5. Performance settings:
   â”œâ”€ Performance mode: General Purpose
   â”œâ”€ Throughput mode: Bursting
   â””â”€ Encryption: Enable encryption at rest

6. File system policy: None (default)

7. Click "Create"

âœ… EFS created! Note the File system ID: fs-12345abc
```

#### Step 2: Update Security Group

```
1. Click on created EFS
2. Go to "Network" tab
3. Click on security group ID

4. Edit inbound rules:
   â”œâ”€ Type: NFS
   â”œâ”€ Protocol: TCP
   â”œâ”€ Port: 2049
   â”œâ”€ Source: Custom â†’ Select default security group
   â””â”€ Description: Allow NFS from EC2

5. Save rules
```

#### Step 3: Launch EC2 Instances

```
INSTANCE 1 (AZ-1a):
1. Go to EC2 â†’ Launch Instance
2. Configuration:
   â”œâ”€ Name: "EFS-Test-Instance-1"
   â”œâ”€ AMI: Amazon Linux 2023
   â”œâ”€ Instance type: t2.micro
   â”œâ”€ Key pair: Create or select existing
   â”œâ”€ Network: Default VPC
   â”œâ”€ Subnet: Choose subnet in us-east-1a
   â”œâ”€ Security group: Default
   â””â”€ Launch

INSTANCE 2 (AZ-1b):
Repeat with:
â”œâ”€ Name: "EFS-Test-Instance-2"
â””â”€ Subnet: Choose subnet in us-east-1b
```

#### Step 4: Connect to Instances

```
USING SSH (LINUX/MAC):
ssh -i "your-key.pem" ec2-user@<instance-public-ip>

USING EC2 INSTANCE CONNECT (BROWSER):
1. Select instance
2. Click "Connect"
3. Choose "EC2 Instance Connect"
4. Click "Connect"
```

#### Step 5: Mount EFS on Both Instances

```
ON EACH INSTANCE:

1. Become root:
sudo su -

2. Install NFS client:
sudo yum install -y amazon-efs-utils

3. Create mount directory:
mkdir /mnt/efs

4. Mount EFS (replace fs-12345abc with your File System ID):
sudo mount -t efs fs-12345abc:/ /mnt/efs

# Or using EFS helper:
sudo mount -t efs -o tls fs-12345abc:/ /mnt/efs

5. Verify mount:
df -h | grep efs

# Output shows EFS mounted on /mnt/efs

6. Change to EFS directory:
cd /mnt/efs
```

#### Step 6: Test Shared File System

```
ON INSTANCE 1:
cd /mnt/efs
echo "Hello from Instance 1" > file1.txt
echo "Shared data" > shared-file.txt
ls -la

ON INSTANCE 2:
cd /mnt/efs
ls -la

# Output shows:
# file1.txt
# shared-file.txt

cat file1.txt
# Output: Hello from Instance 1

# Create file from Instance 2
echo "Hello from Instance 2" > file2.txt

BACK ON INSTANCE 1:
ls -la
# Output shows file2.txt âœ…

cat file2.txt
# Output: Hello from Instance 2

âœ… Files synchronized across both instances!
```

#### Step 7: Make Mount Persistent (Optional)

```
ON EACH INSTANCE:

1. Edit fstab:
sudo nano /etc/fstab

2. Add line (replace fs-12345abc):
fs-12345abc:/ /mnt/efs efs _netdev,tls 0 0

3. Save and exit (Ctrl+X, Y, Enter)

4. Test mount:
sudo umount /mnt/efs
sudo mount -a

5. Verify:
df -h | grep efs

âœ… EFS will auto-mount on reboot
```

---

### METHOD 2: AWS CLI / CLOUDSHELL

#### Step 1: Create EFS File System

```bash
# Get default VPC ID
VPC_ID=$(aws ec2 describe-vpcs \
  --filters "Name=isDefault,Values=true" \
  --query 'Vpcs[0].VpcId' \
  --output text)

# Create EFS file system
EFS_ID=$(aws efs create-file-system \
  --performance-mode generalPurpose \
  --throughput-mode bursting \
  --encrypted \
  --tags Key=Name,Value=shared-efs-cli \
  --query 'FileSystemId' \
  --output text)

echo "EFS File System ID: $EFS_ID"

# Wait for EFS to become available
aws efs describe-file-systems --file-system-id $EFS_ID

# Output shows LifeCycleState: "available"
```

#### Step 2: Create Mount Targets

```bash
# Get subnet IDs
SUBNET_1=$(aws ec2 describe-subnets \
  --filters "Name=vpc-id,Values=$VPC_ID" "Name=availability-zone,Values=us-east-1a" \
  --query 'Subnets[0].SubnetId' \
  --output text)

SUBNET_2=$(aws ec2 describe-subnets \
  --filters "Name=vpc-id,Values=$VPC_ID" "Name=availability-zone,Values=us-east-1b" \
  --query 'Subnets[0].SubnetId' \
  --output text)

# Get default security group
SG_ID=$(aws ec2 describe-security-groups \
  --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=default" \
  --query 'SecurityGroups[0].GroupId' \
  --output text)

# Create mount targets
aws efs create-mount-target \
  --file-system-id $EFS_ID \
  --subnet-id $SUBNET_1 \
  --security-groups $SG_ID

aws efs create-mount-target \
  --file-system-id $EFS_ID \
  --subnet-id $SUBNET_2 \
  --security-groups $SG_ID

# Verify mount targets
aws efs describe-mount-targets --file-system-id $EFS_ID
```

#### Step 3: Update Security Group for NFS

```bash
# Add NFS rule to default security group
aws ec2 authorize-security-group-ingress \
  --group-id $SG_ID \
  --protocol tcp \
  --port 2049 \
  --source-group $SG_ID \
  --region us-east-1

# Verify rule added
aws ec2 describe-security-groups --group-ids $SG_ID
```

#### Step 4: Launch EC2 Instances (Automated)

```bash
# Get Amazon Linux 2023 AMI ID
AMI_ID=$(aws ec2 describe-images \
  --filters "Name=name,Values=al2023-ami-2023*" "Name=architecture,Values=x86_64" \
  --query 'Images | sort_by(@, &CreationDate) | [-1].ImageId' \
  --output text)

# Create key pair (if not exists)
aws ec2 create-key-pair \
  --key-name efs-lab-key \
  --query 'KeyMaterial' \
  --output text > efs-lab-key.pem
chmod 400 efs-lab-key.pem

# Launch Instance 1 (AZ-1a)
INSTANCE_1=$(aws ec2 run-instances \
  --image-id $AMI_ID \
  --instance-type t2.micro \
  --key-name efs-lab-key \
  --subnet-id $SUBNET_1 \
  --security-group-ids $SG_ID \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=EFS-CLI-Instance-1}]' \
  --query 'Instances[0].InstanceId' \
  --output text)

# Launch Instance 2 (AZ-1b)
INSTANCE_2=$(aws ec2 run-instances \
  --image-id $AMI_ID \
  --instance-type t2.micro \
  --key-name efs-lab-key \
  --subnet-id $SUBNET_2 \
  --security-group-ids $SG_ID \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=EFS-CLI-Instance-2}]' \
  --query 'Instances[0].InstanceId' \
  --output text)

echo "Instance 1: $INSTANCE_1"
echo "Instance 2: $INSTANCE_2"

# Wait for instances to run
aws ec2 wait instance-running --instance-ids $INSTANCE_1 $INSTANCE_2
```

#### Step 5: Mount EFS (Manual - via SSH)

```bash
# Get instance public IPs
IP_1=$(aws ec2 describe-instances \
  --instance-ids $INSTANCE_1 \
  --query 'Reservations[0].Instances[0].PublicIpAddress' \
  --output text)

IP_2=$(aws ec2 describe-instances \
  --instance-ids $INSTANCE_2 \
  --query 'Reservations[0].Instances[0].PublicIpAddress' \
  --output text)

echo "Connect to Instance 1: ssh -i efs-lab-key.pem ec2-user@$IP_1"
echo "Connect to Instance 2: ssh -i efs-lab-key.pem ec2-user@$IP_2"

# On each instance, run:
sudo yum install -y amazon-efs-utils
sudo mkdir /mnt/efs
sudo mount -t efs $EFS_ID:/ /mnt/efs
```

---

## ğŸ“ LAB CLEANUP CHECKLIST

### After Each Lab

```
â˜ Delete all S3 objects
â˜ Delete S3 buckets
â˜ Terminate EC2 instances
â˜ Delete EFS file systems
â˜ Delete IAM roles (if created)
â˜ Delete security group rules (if added)
â˜ Delete lifecycle policies
â˜ Delete replication rules
â˜ Verify no resources remain (check billing dashboard)
```

### Cost Monitoring

```bash
# Check S3 storage usage
aws s3 ls --summarize --human-readable --recursive

# List running EC2 instances
aws ec2 describe-instances \
  --filters "Name=instance-state-name,Values=running" \
  --query 'Reservations[].Instances[].[InstanceId,InstanceType,State.Name]' \
  --output table

# Check EFS file systems
aws efs describe-file-systems \
  --query 'FileSystems[].[FileSystemId,Name,SizeInBytes.Value]' \
  --output table
```

---

## ğŸ“ LAB NOTES & TIPS

### Common Errors and Solutions

**Error 1: Bucket name already exists**
- Solution: Use unique suffix (timestamp, random number, initials)

**Error 2: Access Denied when accessing public URL**
- Solution: Check Block Public Access settings, then make object public

**Error 3: EFS mount fails**
- Solution: Check security group allows NFS (port 2049) from instance SG

**Error 4: Replication not working**
- Solution: Ensure versioning enabled on both source and destination buckets

**Error 5: Cannot delete bucket**
- Solution: Empty bucket first, including all versions and delete markers

### Best Practices

âœ… **Naming Convention:** Use descriptive names with date/initials
âœ… **Region Selection:** Use same region for related resources to reduce latency/costs
âœ… **Security:** Never leave buckets or objects public unless necessary
âœ… **Cost Control:** Delete resources immediately after lab completion
âœ… **Documentation:** Take notes on resource IDs and configurations
âœ… **Verification:** Always verify operations completed successfully

---

**Happy Learning! Practice these labs multiple times to build muscle memory.** ğŸš€
