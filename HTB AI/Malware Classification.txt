Step 1: Check Dataset Availability

import os

# Check if malware dataset exists
data_paths = [
    '/data/malimg_paper_dataset_imgs',
    './malimg_paper_dataset_imgs',
    'malimg_paper_dataset_imgs'
]

found = False
for path in data_paths:
    if os.path.exists(path):
        print(f"‚úÖ Found dataset at: {path}")
        DATA_BASE_PATH = path
        found = True
        break

if not found:
    print("‚ùå Dataset not found in expected locations")
    print("Searching /data directory...")
    for root, dirs, files in os.walk('/data'):
        if 'malimg' in root.lower():
            print(f"Found: {root}")

# List malware families
if found:
    families = os.listdir(DATA_BASE_PATH)
    print(f"\nüìä Found {len(families)} malware families:")
    for family in sorted(families)[:10]:
        count = len(os.listdir(os.path.join(DATA_BASE_PATH, family)))
        print(f"  {family}: {count} samples")
    if len(families) > 10:
        print(f"  ... and {len(families) - 10} more")

‚úÖ Found dataset at: ./malimg_paper_dataset_imgs

üìä Found 25 malware families:
  Adialer.C: 122 samples
  Agent.FYI: 116 samples
  Allaple.A: 2949 samples
  Allaple.L: 1591 samples
  Alueron.gen!J: 198 samples
  Autorun.K: 106 samples
  C2LOP.P: 146 samples
  C2LOP.gen!g: 200 samples
  Dialplatform.B: 177 samples
  Dontovo.A: 162 samples
  ... and 15 more


Step 2: Visualize Class Distribution

import matplotlib.pyplot as plt
import seaborn as sns

# Compute class distribution
dist = {}
for mlw_class in os.listdir(DATA_BASE_PATH):
    mlw_dir = os.path.join(DATA_BASE_PATH, mlw_class)
    if os.path.isdir(mlw_dir):
        dist[mlw_class] = len(os.listdir(mlw_dir))

# HTB color palette
htb_green = "#9FEF00"
node_black = "#141D2B"
hacker_grey = "#A4B1CD"

# Plot
classes = list(dist.keys())
frequencies = list(dist.values())

plt.figure(figsize=(12, 8), facecolor=node_black)
sns.barplot(y=classes, x=frequencies, edgecolor="black", orient='h', color=htb_green)
plt.title("Malware Class Distribution", color=htb_green, fontsize=16)
plt.xlabel("Number of Samples", color=htb_green, fontsize=12)
plt.ylabel("Malware Family", color=htb_green, fontsize=12)
plt.xticks(color=hacker_grey)
plt.yticks(color=hacker_grey, fontsize=8)

ax = plt.gca()
ax.set_facecolor(node_black)
ax.spines['bottom'].set_color(hacker_grey)
ax.spines['top'].set_color(node_black)
ax.spines['right'].set_color(node_black)
ax.spines['left'].set_color(hacker_grey)

plt.tight_layout()
plt.show()

print(f"\nTotal samples: {sum(frequencies):,}")
print(f"Average per class: {sum(frequencies)/len(frequencies):.0f}")
print(f"Most common: {max(dist, key=dist.get)} ({dist[max(dist, key=dist.get)]})")
print(f"Least common: {min(dist, key=dist.get)} ({dist[min(dist, key=dist.get)]})")


Total samples: 9,339
Average per class: 374
Most common: Allaple.A (2949)
Least common: Skintrim.N (80)


Step 3: Install Required Package

# Install splitfolders for data splitting
import sys
!{sys.executable} -m pip install split-folders -q

print("‚úÖ split-folders installed successfully!")
‚úÖ split-folders installed successfully!


Step 4: Split Dataset (Train/Test)

import splitfolders

TARGET_BASE_PATH = "./newdata/"
TRAINING_RATIO = 0.8
TEST_RATIO = 1 - TRAINING_RATIO

print("=" * 50)
print("SPLITTING DATASET...")
print("=" * 50)
print(f"Training: {TRAINING_RATIO*100}%")
print(f"Testing:  {TEST_RATIO*100}%")
print("\nThis may take 1-2 minutes...")

# Split the dataset
splitfolders.ratio(
    input=DATA_BASE_PATH,
    output=TARGET_BASE_PATH,
    ratio=(TRAINING_RATIO, 0, TEST_RATIO),  # (train, val, test)
    seed=1337
)

print("\n‚úÖ Dataset split complete!")

# Verify split
train_count = sum([len(files) for r, d, files in os.walk(os.path.join(TARGET_BASE_PATH, 'train'))])
test_count = sum([len(files) for r, d, files in os.walk(os.path.join(TARGET_BASE_PATH, 'test'))])

print(f"\nTraining samples: {train_count:,}")
print(f"Test samples: {test_count:,}")
print(f"Total: {train_count + test_count:,}")

==================================================
SPLITTING DATASET...
==================================================
Training: 80.0%
Testing:  19.999999999999996%

This may take 1-2 minutes...

Copying files: 9339 files [00:28, 332.26 files/s]


‚úÖ Dataset split complete!

Training samples: 7,459
Test samples: 1,880
Total: 9,339


Step 5: Define Data Loading Function

from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder

def load_datasets(base_path, train_batch_size, test_batch_size):
    """
    Load and preprocess malware image datasets.
    
    Args:
        base_path: Path to newdata folder
        train_batch_size: Batch size for training
        test_batch_size: Batch size for testing
    
    Returns:
        train_loader, test_loader, n_classes
    """
    
    # Define preprocessing
    transform = transforms.Compose([
        transforms.Resize((75, 75)),  # Resize to 75x75
        transforms.ToTensor(),         # Convert to tensor
        transforms.Normalize(          # Normalize (ImageNet stats)
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    # Load datasets
    train_dataset = ImageFolder(
        root=os.path.join(base_path, "train"),
        transform=transform
    )
    
    test_dataset = ImageFolder(
        root=os.path.join(base_path, "test"),
        transform=transform
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=train_batch_size,
        shuffle=True,      # Shuffle training data
        num_workers=2      # Parallel data loading
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=test_batch_size,
        shuffle=False,     # Don't shuffle test data
        num_workers=2
    )
    
    n_classes = len(train_dataset.classes)
    
    print(f"‚úÖ Datasets loaded!")
    print(f"   Classes: {n_classes}")
    print(f"   Training samples: {len(train_dataset):,}")
    print(f"   Test samples: {len(test_dataset):,}")
    
    return train_loader, test_loader, n_classes

# Test the function
DATA_PATH = "./newdata/"
TRAIN_BATCH_SIZE = 512
TEST_BATCH_SIZE = 1024

train_loader, test_loader, n_classes = load_datasets(
    DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE
)


‚úÖ Datasets loaded!
   Classes: 25
   Training samples: 7,459
   Test samples: 1,880


Step 6: Visualize Preprocessed Image

import torch

# Get a sample batch
sample_batch = next(iter(train_loader))
sample_image = sample_batch[0][0]  # First image from batch
sample_label = sample_batch[1][0].item()  # Its label

# Get class name
class_names = train_loader.dataset.classes
print(f"Sample belongs to: {class_names[sample_label]}")

# Plot
plt.figure(figsize=(6, 6), facecolor=node_black)
plt.imshow(sample_image.permute(1, 2, 0))  # CHW ‚Üí HWC for display
plt.title(f"Preprocessed Malware Image\n({class_names[sample_label]})", 
          color=htb_green, fontsize=14)
plt.xticks(color=hacker_grey)
plt.yticks(color=hacker_grey)

ax = plt.gca()
ax.set_facecolor(node_black)
for spine in ax.spines.values():
    spine.set_color(hacker_grey if spine.spine_type in ['bottom', 'left'] else node_black)

plt.tight_layout()
plt.show()

Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9124069..0.94936836].

Sample belongs to: C2LOP.P



Step 7: Define the Model

import torch.nn as nn
import torchvision.models as models

HIDDEN_LAYER_SIZE = 1000

class MalwareClassifier(nn.Module):
    def __init__(self, n_classes):
        super(MalwareClassifier, self).__init__()
        
        # Load pre-trained ResNet50
        print("Loading pre-trained ResNet50...")
        self.resnet = models.resnet50(weights='DEFAULT')
        
        # Freeze all parameters (don't train them)
        for param in self.resnet.parameters():
            param.requires_grad = False
        
        # Replace final layer for our task
        num_features = self.resnet.fc.in_features  # 2048
        self.resnet.fc = nn.Sequential(
            nn.Linear(num_features, HIDDEN_LAYER_SIZE),
            nn.ReLU(),
            nn.Linear(HIDDEN_LAYER_SIZE, n_classes)
        )
        
        print(f"‚úÖ Model initialized for {n_classes} classes")
    
    def forward(self, x):
        return self.resnet(x)

# Initialize model
model = MalwareClassifier(n_classes)

# Count trainable parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Statistics:")
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
print(f"  Frozen parameters: {total_params - trainable_params:,}")


Loading pre-trained ResNet50...
‚úÖ Model initialized for 25 classes

Model Statistics:
  Total parameters: 25,582,057
  Trainable parameters: 2,074,025 (8.1%)
  Frozen parameters: 23,508,032


Step 8: Define Training Function

import torch
import time

def train(model, train_loader, n_epochs, verbose=False):
    """
    Train the malware classifier.
    
    Args:
        model: The model to train
        train_loader: DataLoader for training data
        n_epochs: Number of epochs
        verbose: Print progress
    
    Returns:
        training_data: Dictionary with accuracy and loss per epoch
    """
    
    model.train()  # Set to training mode
    
    # Loss function and optimizer
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())
    
    training_data = {"accuracy": [], "loss": []}
    
    print("=" * 50)
    print("TRAINING STARTED")
    print("=" * 50)
    
    for epoch in range(n_epochs):
        running_loss = 0
        n_total = 0
        n_correct = 0
        checkpoint = time.time() * 1000
        
        # Iterate over batches
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            # Forward pass
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Calculate accuracy
            _, predicted = outputs.max(1)
            n_total += labels.size(0)
            n_correct += predicted.eq(labels).sum().item()
            running_loss += loss.item()
            
            # Print progress every 5 batches
            if verbose and batch_idx % 5 == 0:
                current_acc = 100.0 * n_correct / n_total
                print(f"  Batch {batch_idx}/{len(train_loader)}: "
                      f"Loss={loss.item():.4f}, Acc={current_acc:.2f}%", end='\r')
        
        # Epoch statistics
        epoch_loss = running_loss / len(train_loader)
        epoch_duration = int(time.time() * 1000 - checkpoint)
        epoch_accuracy = round(100 * n_correct / n_total, 2)
        
        training_data["accuracy"].append(epoch_accuracy)
        training_data["loss"].append(epoch_loss)
        
        print(f"\n[Epoch {epoch+1}/{n_epochs}] "
              f"Accuracy: {epoch_accuracy:.2f}% | "
              f"Loss: {epoch_loss:.4f} | "
              f"Time: {epoch_duration/1000:.1f}s")
    
    print("\n" + "=" * 50)
    print("TRAINING COMPLETE!")
    print("=" * 50)
    
    return training_data

Step 9: Define Evaluation Functions

def compute_accuracy(n_correct, n_total):
    """Calculate accuracy percentage."""
    return round(100 * n_correct / n_total, 2)

def predict(model, test_data):
    """Make predictions on test data."""
    model.eval()  # Set to evaluation mode
    
    with torch.no_grad():  # Disable gradient calculation
        output = model(test_data)
        _, predicted = torch.max(output.data, 1)
    
    return predicted

def evaluate(model, test_loader):
    """
    Evaluate model on test set.
    
    Args:
        model: Trained model
        test_loader: DataLoader for test data
    
    Returns:
        accuracy: Test accuracy percentage
    """
    
    model.eval()
    n_correct = 0
    n_total = 0
    
    print("=" * 50)
    print("EVALUATING MODEL...")
    print("=" * 50)
    
    with torch.no_grad():
        for data, target in test_loader:
            predicted = predict(model, data)
            n_total += target.size(0)
            n_correct += (predicted == target).sum().item()
    
    accuracy = compute_accuracy(n_correct, n_total)
    
    print(f"‚úÖ Test Accuracy: {accuracy}%")
    print("=" * 50)
    
    return accuracy


Step 10: Define Plotting Functions

def plot(data, title, label, xlabel, ylabel):
    """Generic plotting function with HTB theme."""
    
    htb_green = "#9FEF00"
    node_black = "#141D2B"
    hacker_grey = "#A4B1CD"
    
    plt.figure(figsize=(10, 6), facecolor=node_black)
    plt.plot(range(1, len(data)+1), data, label=label, color=htb_green, linewidth=2)
    plt.title(title, color=htb_green, fontsize=16)
    plt.xlabel(xlabel, color=htb_green, fontsize=12)
    plt.ylabel(ylabel, color=htb_green, fontsize=12)
    plt.xticks(color=hacker_grey)
    plt.yticks(color=hacker_grey)
    plt.grid(True, alpha=0.2, color=hacker_grey)
    
    ax = plt.gca()
    ax.set_facecolor(node_black)
    ax.spines['bottom'].set_color(hacker_grey)
    ax.spines['top'].set_color(node_black)
    ax.spines['right'].set_color(node_black)
    ax.spines['left'].set_color(hacker_grey)
    
    legend = plt.legend(facecolor=node_black, edgecolor=hacker_grey, fontsize=10)
    plt.setp(legend.get_texts(), color=htb_green)
    
    plt.tight_layout()
    plt.show()

def plot_training_accuracy(training_data):
    """Plot training accuracy over epochs."""
    plot(training_data['accuracy'], 
         "Training Accuracy Over Epochs", 
         "Accuracy", "Epoch", "Accuracy (%)")

def plot_training_loss(training_data):
    """Plot training loss over epochs."""
    plot(training_data['loss'], 
         "Training Loss Over Epochs", 
         "Loss", "Epoch", "Loss")

Step 11: Train the Model

# Training parameters
N_EPOCHS = 3  # Start with 3 epochs (adjust if needed)

print("‚ö†Ô∏è  Note: Training may take 10+ minutes per epoch in Playground VM")
print("    On local machine with GPU: ~1-2 minutes per epoch\n")

# Train model
training_information = train(model, train_loader, N_EPOCHS, verbose=True)

# Plot results
plot_training_accuracy(training_information)
plot_training_loss(training_information)

‚ö†Ô∏è  Note: Training may take 10+ minutes per epoch in Playground VM
    On local machine with GPU: ~1-2 minutes per epoch

==================================================
TRAINING STARTED
==================================================
  Batch 0/15: Loss=3.2440, Acc=1.56%


Step 12: Evaluate on Test Set

# Evaluate
test_accuracy = evaluate(model, test_loader)

print(f"\nüéØ Final Test Accuracy: {test_accuracy}%")
print(f"   (Target: >85% for flag)")


Step 13: Save the Model

def save_model(model, path):
    """Save model using TorchScript."""
    model_scripted = torch.jit.script(model)
    model_scripted.save(path)
    print(f"‚úÖ Model saved to: {path}")

# Save
MODEL_FILE = "malware_classifier.pth"
save_model(model, MODEL_FILE)

# Check file size
file_size_mb = os.path.getsize(MODEL_FILE) / (1024 * 1024)
print(f"   File size: {file_size_mb:.1f} MB")

Step 14: Submit for Evaluation

import requests
import json

# Submit model (port 8002 for malware classification!)
url = "http://localhost:8002/api/upload"
model_file_path = "malware_classifier.pth"

print("=" * 50)
print("SUBMITTING MODEL FOR EVALUATION...")
print("=" * 50)
print("‚ö†Ô∏è  This may take up to 2 minutes...\n")

try:
    with open(model_file_path, "rb") as model_file:
        files = {"model": model_file}
        response = requests.post(url, files=files)
    
    result = response.json()
    print(json.dumps(result, indent=4))
    
    if "flag" in result:
        print("\n" + "=" * 50)
        print("üéâ SUCCESS! YOUR FLAG:")
        print("=" * 50)
        print(result['flag'])
        print("=" * 50)
    else:
        print("\n‚ö†Ô∏è  No flag returned.")
        if "accuracy" in result:
            print(f"   Model accuracy: {result['accuracy']}")
            print("   May need to train more epochs if < 85%")
        
except Exception as e:
    print(f"‚ùå Error: {e}")
    print("Make sure evaluation server is running on localhost:8002")


Complete Training Script (All-in-One)
If you want everything in one cell:
python# ============ FULL TRAINING PIPELINE ============

# Parameters
N_EPOCHS = 3
TRAIN_BATCH_SIZE = 512
TEST_BATCH_SIZE = 1024

# Load data
train_loader, test_loader, n_classes = load_datasets(
    "./newdata/", TRAIN_BATCH_SIZE, TEST_BATCH_SIZE
)

# Initialize model
model = MalwareClassifier(n_classes)

# Train
print("\n‚è∞ Training will take approximately:", N_EPOCHS * 10, "minutes in Playground VM")
training_info = train(model, train_loader, N_EPOCHS, verbose=True)

# Evaluate
test_acc = evaluate(model, test_loader)

# Save
save_model(model, "malware_classifier.pth")

# Plot
plot_training_accuracy(training_info)
plot_training_loss(training_info)

print(f"\n‚úÖ All done! Final accuracy: {test_acc}%")
