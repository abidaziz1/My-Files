import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, confusion_matrix, classification_report
)
import seaborn as sns
import matplotlib.pyplot as plt

print("‚úÖ All libraries loaded successfully!")

‚úÖ All libraries loaded successfully!

# Define column names for the NSL-KDD dataset
columns = [
    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 
    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 
    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 
    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 
    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 
    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 
    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 
    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 
    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'
]

# Try multiple possible paths
possible_paths = [
    'KDD+.txt',
    './KDD+.txt',
    '/data/KDD+.txt',
    'KDD_dataset/KDD+.txt'
]

df = None
for file_path in possible_paths:
    try:
        df = pd.read_csv(file_path, names=columns)
        print(f"‚úÖ Dataset loaded from: {file_path}")
        break
    except FileNotFoundError:
        continue

if df is None:
    print("‚ùå Could not find dataset file")
else:
    print("\n" + "=" * 50)
    print("DATASET OVERVIEW:")
    print("=" * 50)
    print(f"Shape: {df.shape}")
    print(f"\nFirst few rows:")
    print(df.head())
    print(f"\nColumn types:")
    print(df.dtypes)
    print(f"\nAttack distribution:")
    print(df['attack'].value_counts().head(10))



‚úÖ Dataset loaded from: KDD+.txt

==================================================
DATASET OVERVIEW:
==================================================
Shape: (148517, 43)

First few rows:
   duration protocol_type   service flag  src_bytes  dst_bytes  land  \
0         0           tcp  ftp_data   SF        491          0     0   
1         0           udp     other   SF        146          0     0   
2         0           tcp   private   S0          0          0     0   
3         0           tcp      http   SF        232       8153     0   
4         0           tcp      http   SF        199        420     0   

   wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \
0               0       0    0  ...                    0.17   
1               0       0    0  ...                    0.00   
2               0       0    0  ...                    0.10   
3               0       0    0  ...                    1.00   
4               0       0    0  ...                    1.00   

   dst_host_diff_srv_rate  dst_host_same_src_port_rate  \
0                    0.03                         0.17   
1                    0.60                         0.88   
2                    0.05                         0.00   
3                    0.00                         0.03   
4                    0.00                         0.00   

   dst_host_srv_diff_host_rate  dst_host_serror_rate  \
0                         0.00                  0.00   
1                         0.00                  0.00   
2                         0.00                  1.00   
3                         0.04                  0.03   
4                         0.00                  0.00   

   dst_host_srv_serror_rate  dst_host_rerror_rate  dst_host_srv_rerror_rate  \
0                      0.00                  0.05                      0.00   
1                      0.00                  0.00                      0.00   
2                      1.00                  0.00                      0.00   
3                      0.01                  0.00                      0.01   
4                      0.00                  0.00                      0.00   

    attack  level  
0   normal     20  
1   normal     15  
2  neptune     19  
3   normal     21  
4   normal     21  

[5 rows x 43 columns]

Column types:
duration                         int64
protocol_type                   object
service                         object
flag                            object
src_bytes                        int64
dst_bytes                        int64
land                             int64
wrong_fragment                   int64
urgent                           int64
hot                              int64
num_failed_logins                int64
logged_in                        int64
num_compromised                  int64
root_shell                       int64
su_attempted                     int64
num_root                         int64
num_file_creations               int64
num_shells                       int64
num_access_files                 int64
num_outbound_cmds                int64
is_host_login                    int64
is_guest_login                   int64
count                            int64
srv_count                        int64
serror_rate                    float64
srv_serror_rate                float64
rerror_rate                    float64
srv_rerror_rate                float64
same_srv_rate                  float64
diff_srv_rate                  float64
srv_diff_host_rate             float64
dst_host_count                   int64
dst_host_srv_count               int64
dst_host_same_srv_rate         float64
dst_host_diff_srv_rate         float64
dst_host_same_src_port_rate    float64
dst_host_srv_diff_host_rate    float64
dst_host_serror_rate           float64
dst_host_srv_serror_rate       float64
dst_host_rerror_rate           float64
dst_host_srv_rerror_rate       float64
attack                          object
level                            int64
dtype: object

Attack distribution:
attack
normal          77054
neptune         45871
satan            4368
ipsweep          3740
smurf            3311
portsweep        3088
nmap             1566
back             1315
guess_passwd     1284
mscan             996
Name: count, dtype: int64

Step 4: Create Classification Targets
4a. Binary Classification (Normal vs Attack)
# Create binary target: 0 = normal, 1 = attack
df['attack_flag'] = df['attack'].apply(lambda a: 0 if a == 'normal' else 1)

print("Binary Classification Target:")
print(df['attack_flag'].value_counts())
print(f"\nNormal: {(df['attack_flag'] == 0).sum()}")
print(f"Attack: {(df['attack_flag'] == 1).sum()}")

Binary Classification Target:
attack_flag
0    77054
1    71463
Name: count, dtype: int64
Normal: 77054
Attack: 71463


4b. Multi-Class Classification (Attack Types)
# Define attack categories
dos_attacks = ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 
               'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm']

probe_attacks = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']

privilege_attacks = ['buffer_overflow', 'loadmdoule', 'perl', 'ps', 
                     'rootkit', 'sqlattack', 'xterm']

access_attacks = ['ftp_write', 'guess_passwd', 'http_tunnel', 'imap', 
                  'multihop', 'named', 'phf', 'sendmail', 'snmpgetattack', 
                  'snmpguess', 'spy', 'warezclient', 'warezmaster', 
                  'xclock', 'xsnoop']

# Mapping function
def map_attack(attack):
    if attack == 'normal':
        return 0  # Normal
    elif attack in dos_attacks:
        return 1  # DoS
    elif attack in probe_attacks:
        return 2  # Probe
    elif attack in privilege_attacks:
        return 3  # Privilege Escalation
    elif attack in access_attacks:
        return 4  # Access
    else:
        return 0  # Unknown ‚Üí treat as normal

# Apply mapping
df['attack_map'] = df['attack'].apply(map_attack)

# Display distribution
print("\n" + "=" * 50)
print("MULTI-CLASS TARGET DISTRIBUTION:")
print("=" * 50)
class_labels = ['Normal', 'DoS', 'Probe', 'Privilege', 'Access']
for i, label in enumerate(class_labels):
    count = (df['attack_map'] == i).sum()
    print(f"{label} ({i}): {count:,} ({count/len(df)*100:.2f}%)")


==================================================
MULTI-CLASS TARGET DISTRIBUTION:
==================================================
Normal (0): 77,207 (51.99%)
DoS (1): 53,387 (35.95%)
Probe (2): 14,077 (9.48%)
Privilege (3): 108 (0.07%)
Access (4): 3,738 (2.52%)


Step 5: Encode Categorical Variables

# One-hot encode protocol_type and service
features_to_encode = ['protocol_type', 'service']
encoded = pd.get_dummies(df[features_to_encode])

print("=" * 50)
print("ENCODED CATEGORICAL FEATURES:")
print("=" * 50)
print(f"Original features: {features_to_encode}")
print(f"Encoded features shape: {encoded.shape}")
print(f"Encoded column names (first 10):")
print(list(encoded.columns[:10]))

==================================================
ENCODED CATEGORICAL FEATURES:
==================================================
Original features: ['protocol_type', 'service']
Encoded features shape: (148517, 73)
Encoded column names (first 10):
['protocol_type_icmp', 'protocol_type_tcp', 'protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier']


Step 6: Select Numeric Features

# Select numeric features
numeric_features = [
    'duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', 
    'num_failed_logins', 'num_compromised', 'root_shell', 'su_attempted', 
    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 
    'num_outbound_cmds', 'count', 'srv_count', 'serror_rate', 
    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 
    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 
    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 
    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 
    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 
    'dst_host_srv_rerror_rate'
]

print("=" * 50)
print("NUMERIC FEATURES:")
print("=" * 50)
print(f"Number of numeric features: {len(numeric_features)}")
print(f"\nSample statistics:")
print(df[numeric_features].describe())

==================================================
NUMERIC FEATURES:
==================================================
Number of numeric features: 34

Sample statistics:
            duration     src_bytes     dst_bytes  wrong_fragment  \
count  148517.000000  1.485170e+05  1.485170e+05   148517.000000   
mean      276.779305  4.022795e+04  1.708885e+04        0.020523   
std      2460.683131  5.409612e+06  3.703525e+06        0.240069   
min         0.000000  0.000000e+00  0.000000e+00        0.000000   
25%         0.000000  0.000000e+00  0.000000e+00        0.000000   
50%         0.000000  4.400000e+01  0.000000e+00        0.000000   
75%         0.000000  2.780000e+02  5.710000e+02        0.000000   
max     57715.000000  1.379964e+09  1.309937e+09        3.000000   

              urgent            hot  num_failed_logins  num_compromised  \
count  148517.000000  148517.000000      148517.000000    148517.000000   
mean        0.000202       0.189379           0.004323         0.255062   
std         0.019417       2.013160           0.072248        22.231375   
min         0.000000       0.000000           0.000000         0.000000   
25%         0.000000       0.000000           0.000000         0.000000   
50%         0.000000       0.000000           0.000000         0.000000   
75%         0.000000       0.000000           0.000000         0.000000   
max         3.000000     101.000000           5.000000      7479.000000   

          root_shell   su_attempted  ...  dst_host_count  dst_host_srv_count  \
count  148517.000000  148517.000000  ...   148517.000000       148517.000000   
mean        0.001508       0.000976  ...      183.928042          119.462661   
std         0.038807       0.042389  ...       98.528328          111.232318   
min         0.000000       0.000000  ...        0.000000            0.000000   
25%         0.000000       0.000000  ...       87.000000           11.000000   
50%         0.000000       0.000000  ...      255.000000           72.000000   
75%         0.000000       0.000000  ...      255.000000          255.000000   
max         1.000000       2.000000  ...      255.000000          255.000000   

       dst_host_same_srv_rate  dst_host_diff_srv_rate  \
count           148517.000000           148517.000000   
mean                 0.534521                0.084103   
std                  0.448061                0.194102   
min                  0.000000                0.000000   
25%                  0.050000                0.000000   
50%                  0.600000                0.020000   
75%                  1.000000                0.070000   
max                  1.000000                1.000000   

       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \
count                148517.000000                148517.000000   
mean                      0.145932                     0.030584   
std                       0.308638                     0.108975   
min                       0.000000                     0.000000   
25%                       0.000000                     0.000000   
50%                       0.000000                     0.000000   
75%                       0.050000                     0.010000   
max                       1.000000                     1.000000   

       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \
count         148517.000000             148517.000000         148517.000000   
mean               0.256122                  0.251304              0.136220   
std                0.428500                  0.429719              0.322741   
min                0.000000                  0.000000              0.000000   
25%                0.000000                  0.000000              0.000000   
50%                0.000000                  0.000000              0.000000   
75%                0.600000                  0.500000              0.000000   
max                1.000000                  1.000000              1.000000   

       dst_host_srv_rerror_rate  
count             148517.000000  
mean                   0.136397  
std                    0.335282  
min                    0.000000  
25%                    0.000000  
50%                    0.000000  
75%                    0.000000  
max                    1.000000  

[8 rows x 34 columns]


Step 7: Combine Features

# Combine encoded categorical + numeric features
train_set = encoded.join(df[numeric_features])

# Multi-class target
multi_y = df['attack_map']

print("=" * 50)
print("FINAL FEATURE SET:")
print("=" * 50)
print(f"Total features: {train_set.shape[1]}")
print(f"Total samples: {train_set.shape[0]}")
print(f"\nFeature set columns (first 20):")
print(list(train_set.columns[:20]))

==================================================
FINAL FEATURE SET:
==================================================
Total features: 107
Total samples: 148517

Feature set columns (first 20):
['protocol_type_icmp', 'protocol_type_tcp', 'protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs']


Step 8: Split the Data

# Split 1: Separate test set (20% of data)
train_X, test_X, train_y, test_y = train_test_split(
    train_set, multi_y, 
    test_size=0.2, 
    random_state=1337,
    stratify=multi_y  # Ensure balanced classes
)

# Split 2: Create validation set from training data (30% of remaining 80%)
multi_train_X, multi_val_X, multi_train_y, multi_val_y = train_test_split(
    train_X, train_y, 
    test_size=0.3, 
    random_state=1337,
    stratify=train_y
)

print("=" * 50)
print("DATA SPLIT:")
print("=" * 50)
print(f"Training set:   {multi_train_X.shape[0]:,} samples ({multi_train_X.shape[0]/len(df)*100:.1f}%)")
print(f"Validation set: {multi_val_X.shape[0]:,} samples ({multi_val_X.shape[0]/len(df)*100:.1f}%)")
print(f"Test set:       {test_X.shape[0]:,} samples ({test_X.shape[0]/len(df)*100:.1f}%)")
print(f"\nTotal: {len(df):,} samples")

==================================================
DATA SPLIT:
==================================================
Training set:   83,169 samples (56.0%)
Validation set: 35,644 samples (24.0%)
Test set:       29,704 samples (20.0%)

Total: 148,517 samples


Step 9: Train the Random Forest Model

# Train Random Forest
print("=" * 50)
print("TRAINING RANDOM FOREST MODEL...")
print("=" * 50)
print("This may take 2-5 minutes depending on dataset size...")

rf_model_multi = RandomForestClassifier(
    n_estimators=100,    # Number of trees
    random_state=1337,
    n_jobs=-1,           # Use all CPU cores
    verbose=1            # Show progress
)

# Train the model
rf_model_multi.fit(multi_train_X, multi_train_y)

print("\n‚úÖ Training complete!")

==================================================
TRAINING RANDOM FOREST MODEL...
==================================================
This may take 2-5 minutes depending on dataset size...

[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.3s


‚úÖ Training complete!

[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.1s finished



Step 10: Evaluate on Validation Set

# Make predictions
multi_predictions = rf_model_multi.predict(multi_val_X)

# Calculate metrics
accuracy = accuracy_score(multi_val_y, multi_predictions)
precision = precision_score(multi_val_y, multi_predictions, average='weighted')
recall = recall_score(multi_val_y, multi_predictions, average='weighted')
f1 = f1_score(multi_val_y, multi_predictions, average='weighted')

print("=" * 50)
print("VALIDATION SET EVALUATION:")
print("=" * 50)
print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")

[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.
[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s

==================================================
VALIDATION SET EVALUATION:
==================================================
Accuracy:  0.9955 (99.55%)
Precision: 0.9955
Recall:    0.9955
F1-Score:  0.9954

[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished



Step 11: Confusion Matrix (Validation)

# Create confusion matrix
conf_matrix = confusion_matrix(multi_val_y, multi_predictions)
class_labels = ['Normal', 'DoS', 'Probe', 'Privilege', 'Access']

# Visualize
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels,
            yticklabels=class_labels)
plt.title('Network Anomaly Detection - Validation Set', fontsize=16)
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Actual', fontsize=12)
plt.tight_layout()
plt.show()

# Classification report
print("\n" + "=" * 50)
print("DETAILED CLASSIFICATION REPORT (Validation):")
print("=" * 50)
print(classification_report(multi_val_y, multi_predictions, target_names=class_labels))


==================================================
DETAILED CLASSIFICATION REPORT (Validation):
==================================================
              precision    recall  f1-score   support

      Normal       0.99      1.00      1.00     18529
         DoS       1.00      1.00      1.00     12813
       Probe       1.00      0.99      1.00      3379
   Privilege       1.00      0.38      0.56        26
      Access       0.96      0.93      0.95       897

    accuracy                           1.00     35644
   macro avg       0.99      0.86      0.90     35644
weighted avg       1.00      1.00      1.00     35644



Step 12: Final Evaluation on Test Set

# Predict on test set
test_multi_predictions = rf_model_multi.predict(test_X)

# Calculate metrics
test_accuracy = accuracy_score(test_y, test_multi_predictions)
test_precision = precision_score(test_y, test_multi_predictions, average='weighted')
test_recall = recall_score(test_y, test_multi_predictions, average='weighted')
test_f1 = f1_score(test_y, test_multi_predictions, average='weighted')

print("\n" + "=" * 50)
print("TEST SET EVALUATION (FINAL):")
print("=" * 50)
print(f"Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"Precision: {test_precision:.4f}")
print(f"Recall:    {test_recall:.4f}")
print(f"F1-Score:  {test_f1:.4f}")

# Confusion Matrix
test_conf_matrix = confusion_matrix(test_y, test_multi_predictions)

plt.figure(figsize=(10, 8))
sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_labels,
            yticklabels=class_labels)
plt.title('Network Anomaly Detection - Test Set', fontsize=16)
plt.xlabel('Predicted', fontsize=12)
plt.ylabel('Actual', fontsize=12)
plt.tight_layout()
plt.show()

# Detailed report
print("\n" + "=" * 50)
print("DETAILED CLASSIFICATION REPORT (Test Set):")
print("=" * 50)
print(classification_report(test_y, test_multi_predictions, target_names=class_labels))


==================================================
TEST SET EVALUATION (FINAL):
==================================================
Accuracy:  0.9947 (99.47%)
Precision: 0.9945
Recall:    0.9947
F1-Score:  0.9945


==================================================
DETAILED CLASSIFICATION REPORT (Test Set):
==================================================
              precision    recall  f1-score   support

      Normal       0.99      1.00      1.00     15442
         DoS       1.00      1.00      1.00     10678
       Probe       0.99      1.00      0.99      2815
   Privilege       0.70      0.33      0.45        21
      Access       0.98      0.90      0.94       748

    accuracy                           0.99     29704
   macro avg       0.93      0.85      0.88     29704
weighted avg       0.99      0.99      0.99     29704

Step 13: Save the Model

import joblib

# Save the model
model_filename = 'network_anomaly_detection_model.joblib'
joblib.dump(rf_model_multi, model_filename)

print("=" * 50)
print(f"‚úÖ Model saved to: {model_filename}")
print("=" * 50)

==================================================
‚úÖ Model saved to: network_anomaly_detection_model.joblib
==================================================

Step 14: Submit for Evaluation

import requests
import json

# Submit to evaluation API (note: port 8001, not 8000!)
url = "http://localhost:8001/api/upload"
model_file_path = "network_anomaly_detection_model.joblib"

print("=" * 50)
print("SUBMITTING MODEL FOR EVALUATION...")
print("=" * 50)

try:
    with open(model_file_path, "rb") as model_file:
        files = {"model": model_file}
        response = requests.post(url, files=files)
    
    # Display result
    result = response.json()
    print("\n" + json.dumps(result, indent=4))
    
    # Extract flag
    if "flag" in result:
        print("\n" + "=" * 50)
        print("üéâ SUCCESS! YOUR FLAG:")
        print("=" * 50)
        print(result['flag'])
        print("=" * 50)
    else:
        print("\n‚ö†Ô∏è No flag returned. Check response for details.")
        
except Exception as e:
    print(f"\n‚ùå Error: {e}")
    print("Make sure evaluation server is running on localhost:8001")

==================================================
SUBMITTING MODEL FOR EVALUATION...
==================================================

{
    "accuracy": 0.9976938550495316,
    "flag": "HTB{n3tw0rk_tr4ff1c_4n0m4ly_d3t3ct0r}"
}

==================================================
üéâ SUCCESS! YOUR FLAG:
==================================================
HTB{n3tw0rk_tr4ff1c_4n0m4ly_d3t3ct0r}
==================================================
